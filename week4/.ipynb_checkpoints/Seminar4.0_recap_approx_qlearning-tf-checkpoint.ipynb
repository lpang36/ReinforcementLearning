{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a __tensorflow__ neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for tensorflow, but you will find it easy to adapt it to almost any python-based deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if os.environ.get(\"DISPLAY\") is str and len(os.environ.get(\"DISPLAY\"))!=0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f24fe8c7290>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGfCS1Kn/D4ZcekiHTaHcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2505209510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as tflayers  # Let's make TF simple again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input variables. We'll support multiple states at once\n",
    "current_states = tf.placeholder(dtype=tf.float32,shape=(None,)+state_dim)\n",
    "actions = tf.placeholder(tf.int32,shape=[None])\n",
    "rewards = tf.placeholder(tf.float32,shape=[None])\n",
    "next_states = tf.placeholder(tf.float32, shape=(None,)+state_dim)\n",
    "is_end = tf.placeholder(tf.bool,shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(l_states, scope=None, reuse=False):\n",
    "    assert l_states.get_shape().as_list() == list((None,)+state_dim)\n",
    "    with tf.variable_scope(scope or \"network\") as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        \n",
    "        # <Your architecture. Please start with a single-layer network>\n",
    "\n",
    "        return l_qvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get q-values for ALL actions in current_states\n",
    "predicted_qvalues = network(current_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = <...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_next_qvalues = network(<...>, reuse=True)\n",
    "gamma = 0.99\n",
    "target_qvalues_for_actions = <target Q-values using rewards and predicted_next_qvalues>\n",
    "target_qvalues_for_actions = tf.where(\n",
    "    is_end, \n",
    "    tf.zeros_like(target_qvalues_for_actions),\n",
    "    target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mean squared error loss function\n",
    "loss = <mean squared between target_qvalues_for_actions and predicted_qvalues_for_actions>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network updates. Note the small learning rate (for stability)\n",
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(\n",
    "    loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"network\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow feature - session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow feature 2 - variables initializer\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can check all your valiables by:\n",
    "# [v.name for v in tf.trainable_variables()]\n",
    "# they should all starts with \"network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inial_epsilon = epsilon = 0.5\n",
    "final_epsilon = 0.01\n",
    "n_epochs = 1000\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        q_values = sess.run(\n",
    "            predicted_qvalues, \n",
    "            feed_dict={current_states:np.array([s])})[0]\n",
    "        \n",
    "        a = <sample action with epsilon-greedy strategy>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #train agent one step. Note that we use one-element arrays instead of scalars \n",
    "        #because that's what function accepts.\n",
    "        curr_loss, _ = sess.run(\n",
    "            ..., \n",
    "            feed_dict={\n",
    "                ...})\n",
    "\n",
    "        total_reward += r\n",
    "        total_loss += curr_loss\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward, total_loss/float(t), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mean reward = 93.500\tepsilon = 0.010\tloss = 0.914\tsteps = 92.500: 100%|██████████| 1000/1000 [1:15:00<00:00,  7.59s/it] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "tr = trange(\n",
    "    n_epochs,\n",
    "    desc=\"mean reward = {:.3f}\\tepsilon = {:.3f}\\tloss = {:.3f}\\tsteps = {:.3f}\".format(0.0, 0.0, 0.0, 0.0),\n",
    "    leave=True)\n",
    "\n",
    "\n",
    "for i in tr:\n",
    "    \n",
    "    sessions = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    session_rewards, session_loss, session_steps = map(np.array, zip(*sessions))\n",
    "    \n",
    "    epsilon -= (inial_epsilon - final_epsilon) / float(n_epochs)\n",
    "    \n",
    "    tr.set_description(\"mean reward = {:.3f}\\tepsilon = {:.3f}\\tloss = {:.3f}\\tsteps = {:.3f}\".format(\n",
    "        np.mean(session_rewards), epsilon, np.mean(session_loss), np.mean(session_steps)))\n",
    "\n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon=0 #Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
