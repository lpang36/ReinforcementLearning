{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a __tensorflow__ neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for tensorflow, but you will find it easy to adapt it to almost any python-based deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if os.environ.get(\"DISPLAY\") is str and len(os.environ.get(\"DISPLAY\"))!=0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-03 03:09:14,421] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd116d10210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd11d2b2950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as tflayers  # Let's make TF simple again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create input variables. We'll support multiple states at once\n",
    "current_states = tf.placeholder(dtype=tf.float32,shape=(None,)+state_dim)\n",
    "actions = tf.placeholder(tf.int32,shape=[None])\n",
    "rewards = tf.placeholder(tf.float32,shape=[None])\n",
    "next_states = tf.placeholder(tf.float32, shape=(None,)+state_dim)\n",
    "is_end = tf.placeholder(tf.bool,shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def network(l_states, scope=None, reuse=False):\n",
    "    assert l_states.get_shape().as_list() == list((None,)+state_dim)\n",
    "    with tf.variable_scope(scope or \"network\") as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        \n",
    "        # <Your architecture. Please start with a single-layer network>\n",
    "\n",
    "        return l_qvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get q-values for ALL actions in current_states\n",
    "predicted_qvalues = network(current_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = <...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_next_qvalues = network(<...>, reuse=True)\n",
    "gamma = 0.99\n",
    "target_qvalues_for_actions = <target Q-values using rewards and predicted_next_qvalues>\n",
    "target_qvalues_for_actions = tf.where(\n",
    "    is_end, \n",
    "    tf.zeros_like(target_qvalues_for_actions),\n",
    "    target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mean squared error loss function\n",
    "loss = <mean squared between target_qvalues_for_actions and predicted_qvalues_for_actions>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#network updates. Note the small learning rate (for stability)\n",
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(\n",
    "    loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"network\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow feature - session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow feature 2 - variables initializer\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can check all your valiables by:\n",
    "# [v.name for v in tf.trainable_variables()]\n",
    "# they should all starts with \"network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inial_epsilon = epsilon = 0.5\n",
    "final_epsilon = 0.01\n",
    "n_epochs = 1000\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        q_values = sess.run(\n",
    "            predicted_qvalues, \n",
    "            feed_dict={current_states:np.array([s])})[0]\n",
    "        \n",
    "        a = <sample action with epsilon-greedy strategy>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #train agent one step. Note that we use one-element arrays instead of scalars \n",
    "        #because that's what function accepts.\n",
    "        curr_loss, _ = sess.run(\n",
    "            ..., \n",
    "            feed_dict={\n",
    "                ...})\n",
    "\n",
    "        total_reward += r\n",
    "        total_loss += curr_loss\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward, total_loss/float(t), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mean reward = 93.500\tepsilon = 0.010\tloss = 0.914\tsteps = 92.500: 100%|██████████| 1000/1000 [1:15:00<00:00,  7.59s/it] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "tr = trange(\n",
    "    n_epochs,\n",
    "    desc=\"mean reward = {:.3f}\\tepsilon = {:.3f}\\tloss = {:.3f}\\tsteps = {:.3f}\".format(0.0, 0.0, 0.0, 0.0),\n",
    "    leave=True)\n",
    "\n",
    "\n",
    "for i in tr:\n",
    "    \n",
    "    sessions = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    session_rewards, session_loss, session_steps = map(np.array, zip(*sessions))\n",
    "    \n",
    "    epsilon -= (inial_epsilon - final_epsilon) / float(n_epochs)\n",
    "    \n",
    "    tr.set_description(\"mean reward = {:.3f}\\tepsilon = {:.3f}\\tloss = {:.3f}\\tsteps = {:.3f}\".format(\n",
    "        np.mean(session_rewards), epsilon, np.mean(session_loss), np.mean(session_steps)))\n",
    "\n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon=0 #Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
