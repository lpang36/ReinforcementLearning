{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning with crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states,n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.full([n_states,n_actions],1./n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray,np.matrix)\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        a = np.random.choice(n_actions,1,p=policy[s,:])[0]\n",
    "\n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward = total_reward+r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,a,r = generate_session()\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) is float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.61 s, sys: 12 ms, total: 2.62 s\n",
      "Wall time: 2.62 s\n",
      "mean reward = -774.85600\tthreshold = -785.0\n",
      "CPU times: user 2.15 s, sys: 0 ns, total: 2.15 s\n",
      "Wall time: 2.15 s\n",
      "mean reward = -706.59200\tthreshold = -722.0\n",
      "CPU times: user 2.04 s, sys: 0 ns, total: 2.04 s\n",
      "Wall time: 2.03 s\n",
      "mean reward = -649.46400\tthreshold = -668.0\n",
      "CPU times: user 1.9 s, sys: 0 ns, total: 1.9 s\n",
      "Wall time: 1.89 s\n",
      "mean reward = -561.72800\tthreshold = -605.0\n",
      "CPU times: user 1.8 s, sys: 0 ns, total: 1.8 s\n",
      "Wall time: 1.79 s\n",
      "mean reward = -484.48400\tthreshold = -533.0\n",
      "CPU times: user 1.55 s, sys: 0 ns, total: 1.55 s\n",
      "Wall time: 1.54 s\n",
      "mean reward = -378.67200\tthreshold = -428.0\n",
      "CPU times: user 1.09 s, sys: 4 ms, total: 1.1 s\n",
      "Wall time: 1.09 s\n",
      "mean reward = -245.30800\tthreshold = -215.5\n",
      "CPU times: user 804 ms, sys: 0 ns, total: 804 ms\n",
      "Wall time: 801 ms\n",
      "mean reward = -186.76800\tthreshold = -132.0\n",
      "CPU times: user 612 ms, sys: 0 ns, total: 612 ms\n",
      "Wall time: 605 ms\n",
      "mean reward = -126.75600\tthreshold = -88.5\n",
      "CPU times: user 496 ms, sys: 4 ms, total: 500 ms\n",
      "Wall time: 495 ms\n",
      "mean reward = -85.61600\tthreshold = -67.0\n",
      "CPU times: user 448 ms, sys: 0 ns, total: 448 ms\n",
      "Wall time: 445 ms\n",
      "mean reward = -70.81600\tthreshold = -46.0\n",
      "CPU times: user 408 ms, sys: 4 ms, total: 412 ms\n",
      "Wall time: 407 ms\n",
      "mean reward = -58.32800\tthreshold = -37.5\n",
      "CPU times: user 388 ms, sys: 0 ns, total: 388 ms\n",
      "Wall time: 388 ms\n",
      "mean reward = -53.62400\tthreshold = -28.0\n",
      "CPU times: user 380 ms, sys: 4 ms, total: 384 ms\n",
      "Wall time: 379 ms\n",
      "mean reward = -49.00800\tthreshold = -20.0\n",
      "CPU times: user 364 ms, sys: 0 ns, total: 364 ms\n",
      "Wall time: 362 ms\n",
      "mean reward = -42.48000\tthreshold = -19.0\n",
      "CPU times: user 380 ms, sys: 4 ms, total: 384 ms\n",
      "Wall time: 383 ms\n",
      "mean reward = -53.22000\tthreshold = -17.0\n",
      "CPU times: user 364 ms, sys: 0 ns, total: 364 ms\n",
      "Wall time: 362 ms\n",
      "mean reward = -44.77200\tthreshold = -15.0\n",
      "CPU times: user 360 ms, sys: 0 ns, total: 360 ms\n",
      "Wall time: 356 ms\n",
      "mean reward = -45.45600\tthreshold = -13.5\n",
      "CPU times: user 344 ms, sys: 0 ns, total: 344 ms\n",
      "Wall time: 345 ms\n",
      "mean reward = -43.61200\tthreshold = -14.0\n",
      "CPU times: user 344 ms, sys: 0 ns, total: 344 ms\n",
      "Wall time: 342 ms\n",
      "mean reward = -49.70400\tthreshold = -13.5\n",
      "CPU times: user 336 ms, sys: 0 ns, total: 336 ms\n",
      "Wall time: 336 ms\n",
      "mean reward = -44.19600\tthreshold = -11.5\n",
      "CPU times: user 340 ms, sys: 0 ns, total: 340 ms\n",
      "Wall time: 339 ms\n",
      "mean reward = -48.28000\tthreshold = -12.5\n",
      "CPU times: user 372 ms, sys: 0 ns, total: 372 ms\n",
      "Wall time: 369 ms\n",
      "mean reward = -56.62400\tthreshold = -10.0\n",
      "CPU times: user 360 ms, sys: 0 ns, total: 360 ms\n",
      "Wall time: 358 ms\n",
      "mean reward = -53.07200\tthreshold = -13.0\n",
      "CPU times: user 324 ms, sys: 0 ns, total: 324 ms\n",
      "Wall time: 319 ms\n",
      "mean reward = -39.90800\tthreshold = -9.0\n",
      "CPU times: user 428 ms, sys: 0 ns, total: 428 ms\n",
      "Wall time: 427 ms\n",
      "mean reward = -84.76400\tthreshold = -19.0\n",
      "CPU times: user 380 ms, sys: 0 ns, total: 380 ms\n",
      "Wall time: 380 ms\n",
      "mean reward = -62.58400\tthreshold = -15.0\n",
      "CPU times: user 448 ms, sys: 0 ns, total: 448 ms\n",
      "Wall time: 444 ms\n",
      "mean reward = -88.43200\tthreshold = -16.0\n",
      "CPU times: user 372 ms, sys: 0 ns, total: 372 ms\n",
      "Wall time: 370 ms\n",
      "mean reward = -61.35200\tthreshold = -9.5\n",
      "CPU times: user 524 ms, sys: 0 ns, total: 524 ms\n",
      "Wall time: 520 ms\n",
      "mean reward = -125.27200\tthreshold = -26.0\n",
      "CPU times: user 524 ms, sys: 0 ns, total: 524 ms\n",
      "Wall time: 518 ms\n",
      "mean reward = -115.32800\tthreshold = -19.5\n",
      "CPU times: user 1.16 s, sys: 4 ms, total: 1.17 s\n",
      "Wall time: 1.17 s\n",
      "mean reward = -142.93200\tthreshold = -24.0\n",
      "CPU times: user 768 ms, sys: 0 ns, total: 768 ms\n",
      "Wall time: 764 ms\n",
      "mean reward = -90.35200\tthreshold = -20.0\n",
      "CPU times: user 624 ms, sys: 4 ms, total: 628 ms\n",
      "Wall time: 627 ms\n",
      "mean reward = -111.07600\tthreshold = -19.5\n",
      "CPU times: user 412 ms, sys: 0 ns, total: 412 ms\n",
      "Wall time: 410 ms\n",
      "mean reward = -76.65200\tthreshold = -15.5\n",
      "CPU times: user 768 ms, sys: 0 ns, total: 768 ms\n",
      "Wall time: 765 ms\n",
      "mean reward = -120.30800\tthreshold = -20.0\n",
      "CPU times: user 512 ms, sys: 4 ms, total: 516 ms\n",
      "Wall time: 511 ms\n",
      "mean reward = -102.13600\tthreshold = -16.5\n",
      "CPU times: user 604 ms, sys: 0 ns, total: 604 ms\n",
      "Wall time: 604 ms\n",
      "mean reward = -92.67200\tthreshold = -18.0\n",
      "CPU times: user 632 ms, sys: 0 ns, total: 632 ms\n",
      "Wall time: 628 ms\n",
      "mean reward = -104.58000\tthreshold = -26.5\n",
      "CPU times: user 460 ms, sys: 0 ns, total: 460 ms\n",
      "Wall time: 458 ms\n",
      "mean reward = -95.64000\tthreshold = -19.0\n",
      "CPU times: user 512 ms, sys: 0 ns, total: 512 ms\n",
      "Wall time: 510 ms\n",
      "mean reward = -69.48800\tthreshold = -18.0\n",
      "CPU times: user 444 ms, sys: 0 ns, total: 444 ms\n",
      "Wall time: 443 ms\n",
      "mean reward = -91.08400\tthreshold = -21.0\n",
      "CPU times: user 624 ms, sys: 0 ns, total: 624 ms\n",
      "Wall time: 620 ms\n",
      "mean reward = -113.34800\tthreshold = -17.5\n",
      "CPU times: user 636 ms, sys: 0 ns, total: 636 ms\n",
      "Wall time: 630 ms\n",
      "mean reward = -95.58800\tthreshold = -21.0\n",
      "CPU times: user 348 ms, sys: 0 ns, total: 348 ms\n",
      "Wall time: 343 ms\n",
      "mean reward = -51.61200\tthreshold = -9.0\n",
      "CPU times: user 468 ms, sys: 0 ns, total: 468 ms\n",
      "Wall time: 466 ms\n",
      "mean reward = -62.81600\tthreshold = -18.0\n",
      "CPU times: user 528 ms, sys: 0 ns, total: 528 ms\n",
      "Wall time: 525 ms\n",
      "mean reward = -55.68400\tthreshold = -13.0\n",
      "CPU times: user 396 ms, sys: 0 ns, total: 396 ms\n",
      "Wall time: 398 ms\n",
      "mean reward = -55.22400\tthreshold = -9.5\n",
      "CPU times: user 340 ms, sys: 0 ns, total: 340 ms\n",
      "Wall time: 339 ms\n",
      "mean reward = -33.74400\tthreshold = -7.5\n",
      "CPU times: user 356 ms, sys: 0 ns, total: 356 ms\n",
      "Wall time: 353 ms\n",
      "mean reward = -52.67200\tthreshold = -14.0\n"
     ]
    }
   ],
   "source": [
    "n_samples = 250  #sample this many samples\n",
    "percentile = 50  #take this percent of session with highest rewards\n",
    "smoothing = 0.1  #add this thing to all counts for stability\n",
    "\n",
    "for i in range(50):\n",
    "    \n",
    "    %time sessions = [generate_session() for i in range(0,n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "    \n",
    "    threshold = np.percentile(batch_rewards,percentile)\n",
    "    \n",
    "    elite_states = [batch_states[i] for i in range(0,n_samples) if batch_rewards[i]>threshold] \n",
    "    elite_actions = [batch_actions[i] for i in range(0,n_samples) if batch_rewards[i]>threshold] \n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #hint on task above: use np.percentile and numpy-style indexing\n",
    "    \n",
    "    #count actions from elite states\n",
    "    elite_counts = np.zeros_like(policy)\n",
    "    \n",
    "    state_count = np.bincount(elite_states,minlength=n_states)\n",
    "    for i in range(0,n_states):\n",
    "        if state_count[i]==0:\n",
    "            action_count = [1./n_actions]*n_actions\n",
    "        else:\n",
    "            action_count = np.bincount(elite_actions,weights=(elite_states==i),minlength=n_actions)+smoothing\n",
    "            action_count = action_count/(state_count[i]+n_actions*smoothing)\n",
    "        elite_counts[i,:] = elite_counts[i,:]+action_count\n",
    "        \n",
    "    policy = elite_counts/np.repeat(np.reshape(np.sum(elite_counts,1),(-1,1)),n_actions,1)\n",
    "    \n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) crossentropy method\n",
    "\n",
    "In this section we will train a neural network policy for continuous action space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe46c7f9f50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4CWpU/8Pbz96SK5M9rkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe471c52950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-bf74fe2c534b>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-bf74fe2c534b>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    a = <sample action with such probabilities>\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0] \n",
    "        \n",
    "        a = <sample action with such probabilities>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "percentile = 70\n",
    "smoothing = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    #generate new sessions\n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "\n",
    "    threshold = <select percentile of your samples>\n",
    "    \n",
    "    elite_states = <select states from sessions where rewards are above threshold>\n",
    "    elite_actions = <select actions from sessions where rewards are above threshold>\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #elite_states: a list of states from top games\n",
    "    #elite_actions: a list of actions from top games\n",
    "    \n",
    "    <fit agent to predict elite_actions(y) from elite_states(X)>\n",
    "\n",
    "\n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part I\n",
    "\n",
    "### Tabular correntropy method\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from -10k to aroung -500 score (+- 500) and stays there. This is in part because taxi-v2 has some hard-coded randomness in the environment. Other reason is that the percentile was chosen poorly.\n",
    "\n",
    "### Tasks\n",
    "- __1.1__ (1 pt) Modify the tabular CEM (CrossEntropyMethod) code to plot distribution of rewards and threshold on each tick.\n",
    "- __1.2__ (2 pts) Find out how the algorithm performance changes if you change different percentile and different n_samples.\n",
    "\n",
    "```<YOUR ANSWER>```\n",
    "\n",
    "\n",
    "- __1.3__ (2 pts) Tune the algorithm to end up with positive average score.\n",
    "- __1.4 bonus__ (1 pt) Try to achieve a distribution where 25% or more samples score above +9.0\n",
    "- __1.5 bonus__ (2 pts) Solve and upload [Taxi-v1](https://gym.openai.com/envs/Taxi-v1) to the openai gym.\n",
    "\n",
    "It's okay to modify the existing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part II\n",
    "\n",
    "### Deep crossentropy method\n",
    "\n",
    "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved (see the link). It's time to upload the result and get to something harder.\n",
    "\n",
    "* if you have any trouble with CartPole-v0 and feel stuck, feel free to ask us or your peers for help.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* __2.1__ Go to the [gym site](http://gym.openai.com/), register and obtain __api key__.\n",
    "* __2.2__ (1 pt) Upload your result to gym via gym.upload (see Results tab above, the line you need is commented)\n",
    "* __2.3__ (3 pts) Pick one of environments: MountainCar-v0 or LunarLander-v2 (or both) and solve it.\n",
    "  * For MountainCar, learn to finish it in __less than 180 steps__\n",
    "  * For LunarLander, learn to get reward of __at least +50__\n",
    "  * See the tips section below, it's kinda important.\n",
    "  \n",
    "  \n",
    "* __2.4__ (1+ pt) Devise a way to speed up training at least 2x against the default version\n",
    "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
    "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
    "  \n",
    "  \n",
    "### Tips\n",
    "* Gym page: [mountaincar](https://gym.openai.com/envs/MountainCar-v0), [lunarlander](https://gym.openai.com/envs/LunarLander-v2)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n",
    "* __Please upload the results to openai gym and send links to all submissions in the e-mail__\n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "* __2.5 bonus__ Try to find a network architecture and training params that solve __both__ environments above (_Points depend on implementation_)\n",
    "\n",
    "* __2.6 bonus__ Solve continuous action space task with `MLPRegressor` or similar.\n",
    "  * [MountainCarContinuous-v0](https://gym.openai.com/envs/MountainCarContinuous-v0), [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2) (4+ points if it works)\n",
    "  \n",
    "* __2.7 bonus__ Use any deep learning framework of your choice to implement policy-gradient (see lectures) on any of those envs (4 +1 per env):\n",
    "  * CartPole-v0\n",
    "  * MountainCar-v0\n",
    "  * LunarLander-v2\n",
    "  * See __tips on policy gradient__ below.\n",
    "  \n",
    "\n",
    "* __2.8 bonus__ take your favorite deep learning framework and try to get above random in [Atari Breakout](https://gym.openai.com/envs/Breakout-v0) with crossentropy method over a convolutional network.\n",
    "  * Expect at least +10 points if you get this up and running, no deadlines apply ! \n",
    "  * __See tips below on where to start, they're cruicially important__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips on policy gradient\n",
    "\n",
    "* The loss function is very similar to crossentropy method. You can get away with using rewards as  __sample_weights__.\n",
    "* If your algorithm converges to a poor strategy, try regularizing with entropy or just somehow prevent agent from picking actions deterministically (e.g. when probs = 0,0,1,0,0)\n",
    "* We will use `lasagne` later in the course so you can try to [learn it](http://lasagne.readthedocs.io/en/latest/user/tutorial.html).\n",
    "* If you don't want to mess with theano just yet, try [keras](https://keras.io/getting-started/sequential-model-guide/) or [mxnet](http://mxnet.io/tutorials/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tips on atari breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's all the pre-processing and tuning done for you in the code below\n",
    "* Once you got it working, it's probably a good idea to pre-train with autoencoder or something\n",
    "* We use last 4 frames as observations to account for ball velocity\n",
    "* The code below requires ```pip install Image``` and ```pip install gym[atari]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from breakout import make_breakout\n",
    "\n",
    "env = make_breakout()\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the initial state\n",
    "s = env.reset()\n",
    "print (s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plot first observation. Only one frame\n",
    "plt.imshow(s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next frame\n",
    "new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after 10 frames\n",
    "for _ in range(10):\n",
    "    new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "< tons of your code here or elsewhere >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
