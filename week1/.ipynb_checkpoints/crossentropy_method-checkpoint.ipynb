{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning with crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states,n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.full([n_states,n_actions],1./n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray,np.matrix)\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        a = np.random.choice(n_actions,1,p=policy[s,:])[0]\n",
    "\n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward = total_reward+r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,a,r = generate_session()\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) is float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.61 s, sys: 12 ms, total: 2.62 s\n",
      "Wall time: 2.62 s\n",
      "mean reward = -774.85600\tthreshold = -785.0\n",
      "CPU times: user 2.15 s, sys: 0 ns, total: 2.15 s\n",
      "Wall time: 2.15 s\n",
      "mean reward = -706.59200\tthreshold = -722.0\n",
      "CPU times: user 2.04 s, sys: 0 ns, total: 2.04 s\n",
      "Wall time: 2.03 s\n",
      "mean reward = -649.46400\tthreshold = -668.0\n",
      "CPU times: user 1.9 s, sys: 0 ns, total: 1.9 s\n",
      "Wall time: 1.89 s\n",
      "mean reward = -561.72800\tthreshold = -605.0\n",
      "CPU times: user 1.8 s, sys: 0 ns, total: 1.8 s\n",
      "Wall time: 1.79 s\n",
      "mean reward = -484.48400\tthreshold = -533.0\n",
      "CPU times: user 1.55 s, sys: 0 ns, total: 1.55 s\n",
      "Wall time: 1.54 s\n",
      "mean reward = -378.67200\tthreshold = -428.0\n",
      "CPU times: user 1.09 s, sys: 4 ms, total: 1.1 s\n",
      "Wall time: 1.09 s\n",
      "mean reward = -245.30800\tthreshold = -215.5\n",
      "CPU times: user 804 ms, sys: 0 ns, total: 804 ms\n",
      "Wall time: 801 ms\n",
      "mean reward = -186.76800\tthreshold = -132.0\n",
      "CPU times: user 612 ms, sys: 0 ns, total: 612 ms\n",
      "Wall time: 605 ms\n",
      "mean reward = -126.75600\tthreshold = -88.5\n",
      "CPU times: user 496 ms, sys: 4 ms, total: 500 ms\n",
      "Wall time: 495 ms\n",
      "mean reward = -85.61600\tthreshold = -67.0\n",
      "CPU times: user 448 ms, sys: 0 ns, total: 448 ms\n",
      "Wall time: 445 ms\n",
      "mean reward = -70.81600\tthreshold = -46.0\n",
      "CPU times: user 408 ms, sys: 4 ms, total: 412 ms\n",
      "Wall time: 407 ms\n",
      "mean reward = -58.32800\tthreshold = -37.5\n",
      "CPU times: user 388 ms, sys: 0 ns, total: 388 ms\n",
      "Wall time: 388 ms\n",
      "mean reward = -53.62400\tthreshold = -28.0\n",
      "CPU times: user 380 ms, sys: 4 ms, total: 384 ms\n",
      "Wall time: 379 ms\n",
      "mean reward = -49.00800\tthreshold = -20.0\n",
      "CPU times: user 364 ms, sys: 0 ns, total: 364 ms\n",
      "Wall time: 362 ms\n",
      "mean reward = -42.48000\tthreshold = -19.0\n",
      "CPU times: user 380 ms, sys: 4 ms, total: 384 ms\n",
      "Wall time: 383 ms\n",
      "mean reward = -53.22000\tthreshold = -17.0\n",
      "CPU times: user 364 ms, sys: 0 ns, total: 364 ms\n",
      "Wall time: 362 ms\n",
      "mean reward = -44.77200\tthreshold = -15.0\n",
      "CPU times: user 360 ms, sys: 0 ns, total: 360 ms\n",
      "Wall time: 356 ms\n",
      "mean reward = -45.45600\tthreshold = -13.5\n",
      "CPU times: user 344 ms, sys: 0 ns, total: 344 ms\n",
      "Wall time: 345 ms\n",
      "mean reward = -43.61200\tthreshold = -14.0\n",
      "CPU times: user 344 ms, sys: 0 ns, total: 344 ms\n",
      "Wall time: 342 ms\n",
      "mean reward = -49.70400\tthreshold = -13.5\n",
      "CPU times: user 336 ms, sys: 0 ns, total: 336 ms\n",
      "Wall time: 336 ms\n",
      "mean reward = -44.19600\tthreshold = -11.5\n",
      "CPU times: user 340 ms, sys: 0 ns, total: 340 ms\n",
      "Wall time: 339 ms\n",
      "mean reward = -48.28000\tthreshold = -12.5\n",
      "CPU times: user 372 ms, sys: 0 ns, total: 372 ms\n",
      "Wall time: 369 ms\n",
      "mean reward = -56.62400\tthreshold = -10.0\n",
      "CPU times: user 360 ms, sys: 0 ns, total: 360 ms\n",
      "Wall time: 358 ms\n",
      "mean reward = -53.07200\tthreshold = -13.0\n",
      "CPU times: user 324 ms, sys: 0 ns, total: 324 ms\n",
      "Wall time: 319 ms\n",
      "mean reward = -39.90800\tthreshold = -9.0\n",
      "CPU times: user 428 ms, sys: 0 ns, total: 428 ms\n",
      "Wall time: 427 ms\n",
      "mean reward = -84.76400\tthreshold = -19.0\n",
      "CPU times: user 380 ms, sys: 0 ns, total: 380 ms\n",
      "Wall time: 380 ms\n",
      "mean reward = -62.58400\tthreshold = -15.0\n",
      "CPU times: user 448 ms, sys: 0 ns, total: 448 ms\n",
      "Wall time: 444 ms\n",
      "mean reward = -88.43200\tthreshold = -16.0\n",
      "CPU times: user 372 ms, sys: 0 ns, total: 372 ms\n",
      "Wall time: 370 ms\n",
      "mean reward = -61.35200\tthreshold = -9.5\n",
      "CPU times: user 524 ms, sys: 0 ns, total: 524 ms\n",
      "Wall time: 520 ms\n",
      "mean reward = -125.27200\tthreshold = -26.0\n",
      "CPU times: user 524 ms, sys: 0 ns, total: 524 ms\n",
      "Wall time: 518 ms\n",
      "mean reward = -115.32800\tthreshold = -19.5\n",
      "CPU times: user 1.16 s, sys: 4 ms, total: 1.17 s\n",
      "Wall time: 1.17 s\n",
      "mean reward = -142.93200\tthreshold = -24.0\n",
      "CPU times: user 768 ms, sys: 0 ns, total: 768 ms\n",
      "Wall time: 764 ms\n",
      "mean reward = -90.35200\tthreshold = -20.0\n",
      "CPU times: user 624 ms, sys: 4 ms, total: 628 ms\n",
      "Wall time: 627 ms\n",
      "mean reward = -111.07600\tthreshold = -19.5\n",
      "CPU times: user 412 ms, sys: 0 ns, total: 412 ms\n",
      "Wall time: 410 ms\n",
      "mean reward = -76.65200\tthreshold = -15.5\n",
      "CPU times: user 768 ms, sys: 0 ns, total: 768 ms\n",
      "Wall time: 765 ms\n",
      "mean reward = -120.30800\tthreshold = -20.0\n",
      "CPU times: user 512 ms, sys: 4 ms, total: 516 ms\n",
      "Wall time: 511 ms\n",
      "mean reward = -102.13600\tthreshold = -16.5\n",
      "CPU times: user 604 ms, sys: 0 ns, total: 604 ms\n",
      "Wall time: 604 ms\n",
      "mean reward = -92.67200\tthreshold = -18.0\n",
      "CPU times: user 632 ms, sys: 0 ns, total: 632 ms\n",
      "Wall time: 628 ms\n",
      "mean reward = -104.58000\tthreshold = -26.5\n",
      "CPU times: user 460 ms, sys: 0 ns, total: 460 ms\n",
      "Wall time: 458 ms\n",
      "mean reward = -95.64000\tthreshold = -19.0\n",
      "CPU times: user 512 ms, sys: 0 ns, total: 512 ms\n",
      "Wall time: 510 ms\n",
      "mean reward = -69.48800\tthreshold = -18.0\n",
      "CPU times: user 444 ms, sys: 0 ns, total: 444 ms\n",
      "Wall time: 443 ms\n",
      "mean reward = -91.08400\tthreshold = -21.0\n",
      "CPU times: user 624 ms, sys: 0 ns, total: 624 ms\n",
      "Wall time: 620 ms\n",
      "mean reward = -113.34800\tthreshold = -17.5\n",
      "CPU times: user 636 ms, sys: 0 ns, total: 636 ms\n",
      "Wall time: 630 ms\n",
      "mean reward = -95.58800\tthreshold = -21.0\n",
      "CPU times: user 348 ms, sys: 0 ns, total: 348 ms\n",
      "Wall time: 343 ms\n",
      "mean reward = -51.61200\tthreshold = -9.0\n",
      "CPU times: user 468 ms, sys: 0 ns, total: 468 ms\n",
      "Wall time: 466 ms\n",
      "mean reward = -62.81600\tthreshold = -18.0\n",
      "CPU times: user 528 ms, sys: 0 ns, total: 528 ms\n",
      "Wall time: 525 ms\n",
      "mean reward = -55.68400\tthreshold = -13.0\n",
      "CPU times: user 396 ms, sys: 0 ns, total: 396 ms\n",
      "Wall time: 398 ms\n",
      "mean reward = -55.22400\tthreshold = -9.5\n",
      "CPU times: user 340 ms, sys: 0 ns, total: 340 ms\n",
      "Wall time: 339 ms\n",
      "mean reward = -33.74400\tthreshold = -7.5\n",
      "CPU times: user 356 ms, sys: 0 ns, total: 356 ms\n",
      "Wall time: 353 ms\n",
      "mean reward = -52.67200\tthreshold = -14.0\n"
     ]
    }
   ],
   "source": [
    "n_samples = 250  #sample this many samples\n",
    "percentile = 50  #take this percent of session with highest rewards\n",
    "smoothing = 0.1  #add this thing to all counts for stability\n",
    "\n",
    "for i in range(50):\n",
    "    \n",
    "    %time sessions = [generate_session() for i in range(0,n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "    \n",
    "    threshold = np.percentile(batch_rewards,percentile)\n",
    "    \n",
    "    elite_states = [batch_states[i] for i in range(0,n_samples) if batch_rewards[i]>threshold] \n",
    "    elite_actions = [batch_actions[i] for i in range(0,n_samples) if batch_rewards[i]>threshold] \n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #hint on task above: use np.percentile and numpy-style indexing\n",
    "    \n",
    "    #count actions from elite states\n",
    "    elite_counts = np.zeros_like(policy)\n",
    "    \n",
    "    state_count = np.bincount(elite_states,minlength=n_states)\n",
    "    for i in range(0,n_states):\n",
    "        if state_count[i]==0:\n",
    "            action_count = [1./n_actions]*n_actions\n",
    "        else:\n",
    "            action_count = np.bincount(elite_actions,weights=(elite_states==i),minlength=n_actions)+smoothing\n",
    "            action_count = action_count/(state_count[i]+n_actions*smoothing)\n",
    "        elite_counts[i,:] = elite_counts[i,:]+action_count\n",
    "        \n",
    "    policy = elite_counts/np.repeat(np.reshape(np.sum(elite_counts,1),(-1,1)),n_actions,1)\n",
    "    \n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) crossentropy method\n",
    "\n",
    "In this section we will train a neural network policy for continuous action space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faa90b85ed0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAeQueHrIhx0xAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faa95f618d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0] \n",
    "        \n",
    "        a = np.random.choice(n_actions,1,p=probs)[0]\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward = 21.16000\tthreshold = 23.0\n",
      "mean reward = 21.80000\tthreshold = 24.0\n",
      "mean reward = 23.93000\tthreshold = 29.0\n",
      "mean reward = 22.85000\tthreshold = 25.0\n",
      "mean reward = 26.39000\tthreshold = 30.6\n",
      "mean reward = 30.84000\tthreshold = 32.3\n",
      "mean reward = 32.77000\tthreshold = 38.0\n",
      "mean reward = 32.23000\tthreshold = 36.3\n",
      "mean reward = 37.92000\tthreshold = 45.0\n",
      "mean reward = 41.03000\tthreshold = 45.3\n",
      "mean reward = 42.18000\tthreshold = 48.0\n",
      "mean reward = 41.27000\tthreshold = 48.0\n",
      "mean reward = 50.85000\tthreshold = 57.3\n",
      "mean reward = 45.89000\tthreshold = 52.0\n",
      "mean reward = 53.00000\tthreshold = 62.3\n",
      "mean reward = 53.57000\tthreshold = 66.0\n",
      "mean reward = 60.27000\tthreshold = 73.3\n",
      "mean reward = 68.05000\tthreshold = 92.0\n",
      "mean reward = 85.13000\tthreshold = 108.3\n",
      "mean reward = 101.32000\tthreshold = 132.6\n",
      "mean reward = 121.26000\tthreshold = 148.2\n",
      "mean reward = 142.63000\tthreshold = 168.3\n",
      "mean reward = 163.14000\tthreshold = 197.6\n",
      "mean reward = 189.14000\tthreshold = 214.3\n",
      "mean reward = 206.40000\tthreshold = 241.0\n",
      "mean reward = 239.23000\tthreshold = 278.6\n",
      "mean reward = 253.65000\tthreshold = 291.6\n",
      "mean reward = 257.90000\tthreshold = 282.3\n",
      "mean reward = 316.44000\tthreshold = 368.3\n",
      "mean reward = 347.48000\tthreshold = 391.9\n",
      "mean reward = 344.85000\tthreshold = 405.6\n",
      "mean reward = 393.77000\tthreshold = 440.3\n",
      "mean reward = 417.50000\tthreshold = 453.6\n",
      "mean reward = 418.05000\tthreshold = 469.0\n",
      "mean reward = 457.62000\tthreshold = 506.4\n",
      "mean reward = 523.93000\tthreshold = 624.7\n",
      "mean reward = 403.22000\tthreshold = 456.3\n",
      "mean reward = 491.96000\tthreshold = 533.2\n",
      "mean reward = 535.24000\tthreshold = 625.0\n",
      "mean reward = 606.91000\tthreshold = 682.6\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "percentile = 70\n",
    "smoothing = 0.01\n",
    "\n",
    "for i in range(40):\n",
    "    #generate new sessions\n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "\n",
    "    threshold = np.percentile(batch_rewards,percentile)\n",
    "    \n",
    "    elite_states = [batch_states[i] for i in range(0,n_samples) if batch_rewards[i]>threshold] \n",
    "    elite_actions = [batch_actions[i] for i in range(0,n_samples) if batch_rewards[i]>threshold] \n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #elite_states: a list of states from top games\n",
    "    #elite_actions: a list of actions from top games\n",
    "    \n",
    "    agent.fit(elite_states,elite_actions)\n",
    "\n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part I\n",
    "\n",
    "### Tabular correntropy method\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from -10k to aroung -500 score (+- 500) and stays there. This is in part because taxi-v2 has some hard-coded randomness in the environment. Other reason is that the percentile was chosen poorly.\n",
    "\n",
    "### Tasks\n",
    "- __1.1__ (1 pt) Modify the tabular CEM (CrossEntropyMethod) code to plot distribution of rewards and threshold on each tick.\n",
    "- __1.2__ (2 pts) Find out how the algorithm performance changes if you change different percentile and different n_samples.\n",
    "\n",
    "```<YOUR ANSWER>```\n",
    "\n",
    "\n",
    "- __1.3__ (2 pts) Tune the algorithm to end up with positive average score.\n",
    "- __1.4 bonus__ (1 pt) Try to achieve a distribution where 25% or more samples score above +9.0\n",
    "- __1.5 bonus__ (2 pts) Solve and upload [Taxi-v1](https://gym.openai.com/envs/Taxi-v1) to the openai gym.\n",
    "\n",
    "It's okay to modify the existing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part II\n",
    "\n",
    "### Deep crossentropy method\n",
    "\n",
    "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved (see the link). It's time to upload the result and get to something harder.\n",
    "\n",
    "* if you have any trouble with CartPole-v0 and feel stuck, feel free to ask us or your peers for help.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* __2.1__ Go to the [gym site](http://gym.openai.com/), register and obtain __api key__.\n",
    "* __2.2__ (1 pt) Upload your result to gym via gym.upload (see Results tab above, the line you need is commented)\n",
    "* __2.3__ (3 pts) Pick one of environments: MountainCar-v0 or LunarLander-v2 (or both) and solve it.\n",
    "  * For MountainCar, learn to finish it in __less than 180 steps__\n",
    "  * For LunarLander, learn to get reward of __at least +50__\n",
    "  * See the tips section below, it's kinda important.\n",
    "  \n",
    "  \n",
    "* __2.4__ (1+ pt) Devise a way to speed up training at least 2x against the default version\n",
    "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
    "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
    "  \n",
    "  \n",
    "### Tips\n",
    "* Gym page: [mountaincar](https://gym.openai.com/envs/MountainCar-v0), [lunarlander](https://gym.openai.com/envs/LunarLander-v2)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n",
    "* __Please upload the results to openai gym and send links to all submissions in the e-mail__\n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "* __2.5 bonus__ Try to find a network architecture and training params that solve __both__ environments above (_Points depend on implementation_)\n",
    "\n",
    "* __2.6 bonus__ Solve continuous action space task with `MLPRegressor` or similar.\n",
    "  * [MountainCarContinuous-v0](https://gym.openai.com/envs/MountainCarContinuous-v0), [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2) (4+ points if it works)\n",
    "  \n",
    "* __2.7 bonus__ Use any deep learning framework of your choice to implement policy-gradient (see lectures) on any of those envs (4 +1 per env):\n",
    "  * CartPole-v0\n",
    "  * MountainCar-v0\n",
    "  * LunarLander-v2\n",
    "  * See __tips on policy gradient__ below.\n",
    "  \n",
    "\n",
    "* __2.8 bonus__ take your favorite deep learning framework and try to get above random in [Atari Breakout](https://gym.openai.com/envs/Breakout-v0) with crossentropy method over a convolutional network.\n",
    "  * Expect at least +10 points if you get this up and running, no deadlines apply ! \n",
    "  * __See tips below on where to start, they're cruicially important__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips on policy gradient\n",
    "\n",
    "* The loss function is very similar to crossentropy method. You can get away with using rewards as  __sample_weights__.\n",
    "* If your algorithm converges to a poor strategy, try regularizing with entropy or just somehow prevent agent from picking actions deterministically (e.g. when probs = 0,0,1,0,0)\n",
    "* We will use `lasagne` later in the course so you can try to [learn it](http://lasagne.readthedocs.io/en/latest/user/tutorial.html).\n",
    "* If you don't want to mess with theano just yet, try [keras](https://keras.io/getting-started/sequential-model-guide/) or [mxnet](http://mxnet.io/tutorials/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tips on atari breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's all the pre-processing and tuning done for you in the code below\n",
    "* Once you got it working, it's probably a good idea to pre-train with autoencoder or something\n",
    "* We use last 4 frames as observations to account for ball velocity\n",
    "* The code below requires ```pip install Image``` and ```pip install gym[atari]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from breakout import make_breakout\n",
    "\n",
    "env = make_breakout()\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the initial state\n",
    "s = env.reset()\n",
    "print (s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plot first observation. Only one frame\n",
    "plt.imshow(s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next frame\n",
    "new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after 10 frames\n",
    "for _ in range(10):\n",
    "    new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "< tons of your code here or elsewhere >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
